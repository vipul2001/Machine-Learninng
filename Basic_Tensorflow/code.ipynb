{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Operation in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n",
      "tf.Tensor([4 5 6], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3], dtype=np.int32)\n",
    "b = [4, 5, 6]\n",
    "\n",
    "t_a = tf.convert_to_tensor(a)\n",
    "t_b = tf.convert_to_tensor(b)\n",
    "\n",
    "print(t_a)\n",
    "print(t_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "t_ones = tf.ones((2, 3))\n",
    "\n",
    "print(t_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t1 = tf.random.uniform(shape=(5, 2), \n",
    "                       minval=-1.0,\n",
    "                       maxval=1.0)\n",
    "\n",
    "t2 = tf.random.normal(shape=(5, 2), \n",
    "                      mean=0.0,\n",
    "                      stddev=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.449, -0.345],\n",
       "       [ 0.345,  0.171],\n",
       "       [-0.047,  0.674],\n",
       "       [ 0.708,  0.837],\n",
       "       [-0.801, -0.77 ]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.105  0.009]\n",
      " [ 0.856  0.097]\n",
      " [-0.067 -0.734]\n",
      " [-1.044  0.136]\n",
      " [-0.229  0.065]]\n"
     ]
    }
   ],
   "source": [
    "t3 = tf.multiply(t1, t2).numpy()\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=19, shape=(5, 2), dtype=float32, numpy=\n",
       "array([[-0.684, -0.37 ],\n",
       "       [ 2.829,  0.735],\n",
       "       [ 1.385, -0.416],\n",
       "       [-0.766,  0.999],\n",
       "       [-0.516, -0.854]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1+t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting tensors in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.165 0.901 0.631 0.435 0.292 0.643]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.165, 0.901], dtype=float32),\n",
       " array([0.631, 0.435], dtype=float32),\n",
       " array([0.292, 0.643], dtype=float32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "\n",
    "t = tf.random.uniform((6,))\n",
    "\n",
    "print(t.numpy())\n",
    "\n",
    "t_splits = tf.split(t, 3)\n",
    "\n",
    "[item.numpy() for item in t_splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.165 0.901 0.631 0.435 0.292]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.165, 0.901, 0.631], dtype=float32),\n",
       " array([0.435, 0.292], dtype=float32)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "t = tf.random.uniform((5,))\n",
    "\n",
    "print(t.numpy())\n",
    "\n",
    "t_splits = tf.split(t, num_or_size_splits=[3, 2])\n",
    "\n",
    "[item.numpy() for item in t_splits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Tensorflow Data Set from existing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (), types: tf.float32>\n"
     ]
    }
   ],
   "source": [
    "a = [1.2, 3.4, 7.5, 4.1, 5.0, 1.0]\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices(a)\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.2, shape=(), dtype=float32)\n",
      "tf.Tensor(3.4, shape=(), dtype=float32)\n",
      "tf.Tensor(7.5, shape=(), dtype=float32)\n",
      "tf.Tensor(4.1, shape=(), dtype=float32)\n",
      "tf.Tensor(5.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for item in ds:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1: [1.2 3.4 7.5]\n",
      "batch 2: [4.1 5.  1. ]\n"
     ]
    }
   ],
   "source": [
    "ds_batch = ds.batch(3)\n",
    "\n",
    "for i, elem in enumerate(ds_batch, 1):\n",
    "    print('batch {}:'.format(i), elem.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "\n",
    "t_x = tf.random.uniform([4, 3], dtype=tf.float32)\n",
    "t_y = tf.range(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  x:  [0.165 0.901 0.631]   y:  0\n",
      "  x:  [0.435 0.292 0.643]   y:  1\n",
      "  x:  [0.976 0.435 0.66 ]   y:  2\n",
      "  x:  [0.605 0.637 0.614]   y:  3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ds_x = tf.data.Dataset.from_tensor_slices(t_x)\n",
    "ds_y = tf.data.Dataset.from_tensor_slices(t_y)\n",
    "    \n",
    "ds_joint = tf.data.Dataset.zip((ds_x, ds_y))\n",
    "\n",
    "for example in ds_joint:\n",
    "    print('  x: ', example[0].numpy(), \n",
    "          '  y: ', example[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  x:  [0.165 0.901 0.631]   y:  0\n",
      "  x:  [0.435 0.292 0.643]   y:  1\n",
      "  x:  [0.976 0.435 0.66 ]   y:  2\n",
      "  x:  [0.605 0.637 0.614]   y:  3\n"
     ]
    }
   ],
   "source": [
    "ds_joint = tf.data.Dataset.from_tensor_slices((t_x, t_y))\n",
    "\n",
    "for example in ds_joint:\n",
    "    print('  x: ', example[0].numpy(), \n",
    "          '  y: ', example[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  x:  [-0.67   0.803  0.262]   y:  0\n",
      "  x:  [-0.131 -0.416  0.285]   y:  1\n",
      "  x:  [ 0.952 -0.13   0.32 ]   y:  2\n",
      "  x:  [0.21  0.273 0.229]   y:  3\n"
     ]
    }
   ],
   "source": [
    "ds_trans = ds_joint.map(lambda x, y: (x*2-1.0, y))\n",
    "\n",
    "for example in ds_trans:\n",
    "    print('  x: ', example[0].numpy(), \n",
    "          '  y: ', example[1].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Dataset from available tensorflow Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n",
      "['abstract_reasoning', 'aeslc', 'aflw2k3d', 'amazon_us_reviews', 'bair_robot_pushing_small']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(len(tfds.list_builders()))\n",
    "print(tfds.list_builders()[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract_reasoning',\n",
       " 'aeslc',\n",
       " 'aflw2k3d',\n",
       " 'amazon_us_reviews',\n",
       " 'bair_robot_pushing_small',\n",
       " 'big_patent',\n",
       " 'bigearthnet',\n",
       " 'billsum',\n",
       " 'binarized_mnist',\n",
       " 'binary_alpha_digits',\n",
       " 'c4',\n",
       " 'caltech101',\n",
       " 'caltech_birds2010',\n",
       " 'caltech_birds2011',\n",
       " 'cars196',\n",
       " 'cassava',\n",
       " 'cats_vs_dogs',\n",
       " 'celeb_a',\n",
       " 'celeb_a_hq',\n",
       " 'chexpert',\n",
       " 'cifar10',\n",
       " 'cifar100',\n",
       " 'cifar10_1',\n",
       " 'cifar10_corrupted',\n",
       " 'citrus_leaves',\n",
       " 'clevr',\n",
       " 'cmaterdb',\n",
       " 'cnn_dailymail',\n",
       " 'coco',\n",
       " 'coil100',\n",
       " 'colorectal_histology',\n",
       " 'colorectal_histology_large',\n",
       " 'curated_breast_imaging_ddsm',\n",
       " 'cycle_gan',\n",
       " 'deep_weeds',\n",
       " 'definite_pronoun_resolution',\n",
       " 'diabetic_retinopathy_detection',\n",
       " 'dmlab',\n",
       " 'downsampled_imagenet',\n",
       " 'dsprites',\n",
       " 'dtd',\n",
       " 'duke_ultrasound',\n",
       " 'dummy_dataset_shared_generator',\n",
       " 'dummy_mnist',\n",
       " 'emnist',\n",
       " 'esnli',\n",
       " 'eurosat',\n",
       " 'fashion_mnist',\n",
       " 'flores',\n",
       " 'food101',\n",
       " 'gap',\n",
       " 'gigaword',\n",
       " 'glue',\n",
       " 'groove',\n",
       " 'higgs',\n",
       " 'horses_or_humans',\n",
       " 'i_naturalist2017',\n",
       " 'image_label_folder',\n",
       " 'imagenet2012',\n",
       " 'imagenet2012_corrupted',\n",
       " 'imagenet_resized',\n",
       " 'imdb_reviews',\n",
       " 'iris',\n",
       " 'kitti',\n",
       " 'kmnist',\n",
       " 'lfw',\n",
       " 'lm1b',\n",
       " 'lost_and_found',\n",
       " 'lsun',\n",
       " 'malaria',\n",
       " 'math_dataset',\n",
       " 'mnist',\n",
       " 'mnist_corrupted',\n",
       " 'moving_mnist',\n",
       " 'multi_news',\n",
       " 'multi_nli',\n",
       " 'multi_nli_mismatch',\n",
       " 'newsroom',\n",
       " 'nsynth',\n",
       " 'omniglot',\n",
       " 'open_images_v4',\n",
       " 'oxford_flowers102',\n",
       " 'oxford_iiit_pet',\n",
       " 'para_crawl',\n",
       " 'patch_camelyon',\n",
       " 'pet_finder',\n",
       " 'places365_small',\n",
       " 'plant_leaves',\n",
       " 'plant_village',\n",
       " 'plantae_k',\n",
       " 'quickdraw_bitmap',\n",
       " 'reddit_tifu',\n",
       " 'resisc45',\n",
       " 'rock_paper_scissors',\n",
       " 'rock_you',\n",
       " 'scene_parse150',\n",
       " 'scicite',\n",
       " 'scientific_papers',\n",
       " 'shapes3d',\n",
       " 'smallnorb',\n",
       " 'snli',\n",
       " 'so2sat',\n",
       " 'squad',\n",
       " 'stanford_dogs',\n",
       " 'stanford_online_products',\n",
       " 'starcraft_video',\n",
       " 'sun397',\n",
       " 'super_glue',\n",
       " 'svhn_cropped',\n",
       " 'ted_hrlr_translate',\n",
       " 'ted_multi_translate',\n",
       " 'tf_flowers',\n",
       " 'the300w_lp',\n",
       " 'titanic',\n",
       " 'trivia_qa',\n",
       " 'uc_merced',\n",
       " 'ucf101',\n",
       " 'visual_domain_decathlon',\n",
       " 'voc',\n",
       " 'wider_face',\n",
       " 'wikihow',\n",
       " 'wikipedia',\n",
       " 'wmt14_translate',\n",
       " 'wmt15_translate',\n",
       " 'wmt16_translate',\n",
       " 'wmt17_translate',\n",
       " 'wmt18_translate',\n",
       " 'wmt19_translate',\n",
       " 'wmt_t2t_translate',\n",
       " 'wmt_translate',\n",
       " 'xnli',\n",
       " 'xsum']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.list_builders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeaturesDict({\n",
      "    'attributes': FeaturesDict({\n",
      "        '5_o_Clock_Shadow': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Arched_Eyebrows': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Attractive': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Bags_Under_Eyes': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Bald': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Bangs': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Big_Lips': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Big_Nose': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Black_Hair': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Blond_Hair': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Blurry': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Brown_Hair': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Bushy_Eyebrows': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Chubby': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Double_Chin': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Eyeglasses': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Goatee': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Gray_Hair': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Heavy_Makeup': Tensor(shape=(), dtype=tf.bool),\n",
      "        'High_Cheekbones': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Male': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Mouth_Slightly_Open': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Mustache': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Narrow_Eyes': Tensor(shape=(), dtype=tf.bool),\n",
      "        'No_Beard': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Oval_Face': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Pale_Skin': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Pointy_Nose': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Receding_Hairline': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Rosy_Cheeks': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Sideburns': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Smiling': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Straight_Hair': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Wavy_Hair': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Wearing_Earrings': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Wearing_Hat': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Wearing_Lipstick': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Wearing_Necklace': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Wearing_Necktie': Tensor(shape=(), dtype=tf.bool),\n",
      "        'Young': Tensor(shape=(), dtype=tf.bool),\n",
      "    }),\n",
      "    'image': Image(shape=(218, 178, 3), dtype=tf.uint8),\n",
      "    'landmarks': FeaturesDict({\n",
      "        'lefteye_x': Tensor(shape=(), dtype=tf.int64),\n",
      "        'lefteye_y': Tensor(shape=(), dtype=tf.int64),\n",
      "        'leftmouth_x': Tensor(shape=(), dtype=tf.int64),\n",
      "        'leftmouth_y': Tensor(shape=(), dtype=tf.int64),\n",
      "        'nose_x': Tensor(shape=(), dtype=tf.int64),\n",
      "        'nose_y': Tensor(shape=(), dtype=tf.int64),\n",
      "        'righteye_x': Tensor(shape=(), dtype=tf.int64),\n",
      "        'righteye_y': Tensor(shape=(), dtype=tf.int64),\n",
      "        'rightmouth_x': Tensor(shape=(), dtype=tf.int64),\n",
      "        'rightmouth_y': Tensor(shape=(), dtype=tf.int64),\n",
      "    }),\n",
      "})\n",
      "\n",
      " ============================== \n",
      "\n",
      "dict_keys(['image', 'landmarks', 'attributes'])\n",
      "\n",
      " ============================== \n",
      "\n",
      "Image(shape=(218, 178, 3), dtype=tf.uint8)\n",
      "\n",
      " ============================== \n",
      "\n",
      "dict_keys(['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young'])\n",
      "\n",
      " ============================== \n",
      "\n",
      "@inproceedings{conf/iccv/LiuLWT15,\n",
      "  added-at = {2018-10-09T00:00:00.000+0200},\n",
      "  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},\n",
      "  biburl = {https://www.bibsonomy.org/bibtex/250e4959be61db325d2f02c1d8cd7bfbb/dblp},\n",
      "  booktitle = {ICCV},\n",
      "  crossref = {conf/iccv/2015},\n",
      "  ee = {http://doi.ieeecomputersociety.org/10.1109/ICCV.2015.425},\n",
      "  interhash = {3f735aaa11957e73914bbe2ca9d5e702},\n",
      "  intrahash = {50e4959be61db325d2f02c1d8cd7bfbb},\n",
      "  isbn = {978-1-4673-8391-2},\n",
      "  keywords = {dblp},\n",
      "  pages = {3730-3738},\n",
      "  publisher = {IEEE Computer Society},\n",
      "  timestamp = {2018-10-11T11:43:28.000+0200},\n",
      "  title = {Deep Learning Face Attributes in the Wild.},\n",
      "  url = {http://dblp.uni-trier.de/db/conf/iccv/iccv2015.html#LiuLWT15},\n",
      "  year = 2015\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "celeba_bldr = tfds.builder('celeb_a')\n",
    "\n",
    "print(celeba_bldr.info.features)\n",
    "print('\\n', 30*\"=\", '\\n')\n",
    "print(celeba_bldr.info.features.keys())\n",
    "print('\\n', 30*\"=\", '\\n')\n",
    "print(celeba_bldr.info.features['image'])\n",
    "print('\\n', 30*\"=\", '\\n')\n",
    "print(celeba_bldr.info.features['attributes'].keys())\n",
    "print('\\n', 30*\"=\", '\\n')\n",
    "print(celeba_bldr.info.citation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using celeb Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset celeb_a (1.38 GiB) to C:\\Users\\vipul\\tensorflow_datasets\\celeb_a\\0.3.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c5cf01f9af440d85b4ca98d0231b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Completed...', max=1, style=ProgressStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77321721d6e348d6922e1f547de19aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Size...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e0b8abf29342e2ab7a244841851f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Extraction completed...', max=1, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Download the data, prepare it, and write it to disk\n",
    "celeba_bldr.download_and_prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = celeba_bldr.as_dataset(shuffle_files=False)\n",
    "\n",
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = datasets['train']\n",
    "assert isinstance(ds_train, tf.data.Dataset)\n",
    "\n",
    "example = next(iter(ds_train))\n",
    "print(type(example))\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train.map(lambda item: \n",
    "     (item['image'], tf.cast(item['attributes']['Male'], tf.int32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds_train = ds_train.batch(18)\n",
    "images, labels = next(iter(ds_train))\n",
    "\n",
    "print(images.shape, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "for i,(image,label) in enumerate(zip(images, labels)):\n",
    "    ax = fig.add_subplot(3, 6, i+1)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(image)\n",
    "    ax.set_title('{}'.format(label), size=15)\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
